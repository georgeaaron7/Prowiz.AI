{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdb7363",
   "metadata": {},
   "source": [
    "# BM25: Best Matching 25\n",
    "\n",
    "It is a ranking function used by search engines to estimate the relevance of documents to a given search query. It is a modified version of TF-IDF, taking into consideration term frequency saturation and document length normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914d160",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "- **TF-IDF** gives higher scores to documents containing more occurrences of the query terms, but it does not take into consideration for diminishing returns (i.e., after a certain point, more occurrences of a word do not add much value).\n",
    "- **BM25** solves this by introducing parameters that control the affect of term frequency and document length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa96622",
   "metadata": {},
   "source": [
    "**Inverse Document Frequency (IDF):**\n",
    "\n",
    "$$\n",
    "\\text{IDF}(q_i) = \\log\\left(\\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ = total number of documents\n",
    "- $n(q_i)$ = number of documents containing term $q_i$\n",
    "\n",
    "\n",
    "**BM25 Score:**\n",
    "\n",
    "$$\n",
    "\\text{BM25}(D, Q) = \\sum_{q_i \\in Q} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $f(q_i, D)$ = frequency of $q_i$ in document $D$\n",
    "- $|D|$ = length of document $D$\n",
    "- $\\text{avgdl}$ = average document length\n",
    "- $k_1, b$ = parameters (commonly $k_1=1.5$, $b=0.75$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "546d2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "docs = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the cat lay on the rug\",\n",
    "    \"the dog barked at the cat\"\n",
    "]\n",
    "\n",
    "query = \"cat on mat\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51150bf1",
   "metadata": {},
   "source": [
    "## Tokenize Documents and Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e28bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = [doc.split() for doc in docs]\n",
    "\n",
    "k1 = 1.5\n",
    "b = 0.75\n",
    "N = len(docs)\n",
    "\n",
    "doc_lens = [len(doc) for doc in tokenized_docs]\n",
    "avgdl = sum(doc_lens) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8235355c",
   "metadata": {},
   "source": [
    "## Compute IDF for Each Query Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26d148c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(term):\n",
    "    return sum(1 for doc in tokenized_docs if term in doc)\n",
    "\n",
    "idf = {}\n",
    "for term in query:\n",
    "    n_t = doc_freq(term)\n",
    "    idf[term] = math.log((N - n_t + 0.5) / (n_t + 0.5) + 1e-10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e940bd",
   "metadata": {},
   "source": [
    "## Term Frequency and BM25 Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b27aced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term, doc):\n",
    "    return doc.count(term)\n",
    "\n",
    "def bm25_score(doc):\n",
    "    score = 0\n",
    "    doc_len = len(doc)\n",
    "    for term in query:\n",
    "        f = term_freq(term, doc)\n",
    "        numerator = f * (k1 + 1)\n",
    "        denominator = f + k1 * (1 - b + b * (doc_len / avgdl))\n",
    "        score += idf[term] * (numerator / denominator) if denominator != 0 else 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b5154",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The BM25 scores printed for each document shows how relevant each document is to the query \"cat on mat\". The document with the highest score is considered the most relevant. BM25 takes into consideration not just the presence of query terms, but also their frequency and the length of each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11504217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1 BM25 score: -1.9459\n",
      "Doc 2 BM25 score: -2.4567\n",
      "Doc 3 BM25 score: -1.9459\n"
     ]
    }
   ],
   "source": [
    "scores = [bm25_score(doc) for doc in tokenized_docs]\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"Doc {i+1} BM25 score: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
